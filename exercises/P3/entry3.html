<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Exercise 3 | Entry 3</title>
  <style>
    body {
      font-family: 'Verdana', sans-serif;
      max-width: 800px;
      margin: auto;
      padding: 20px;
      background-color: #d2baee;
      line-height: 1.6;
    }
    .container {
      background-color: white;
      padding: 20px;
      border-radius: 10px;
      box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
    }
    h1, h2 {
      color: #450549;
      text-align: center;
    }
    p {
      font-size: 18px;
      text-align: justify;
    }
    pre {
      background-color: #f4f4f4;
      padding: 10px;
      overflow-x: auto;
      border-radius: 5px;
      font-size: 16px;
    }
    img {
      width: 100%;
      max-width: 600px;
      display: block;
      margin: 15px auto;
      border-radius: 10px;
    }
    .back {
      display: block;
      margin-top: 20px;
      text-align: center;
      font-size: 16px;
      font-weight: bold;
      color: #b82c89;
      text-decoration: none;
    }
    .back:hover {
      color: #b82c89;
    }
  </style>
</head>
<body>
  <div class="container">
    <h1>Entry 3: Correcting Global Pose Estimation</h1>

    <h2>‚ùå Intrinsic Matrix Error</h2>
    <p>
        After looking at posts in the forum, I have seen that it had been indicated that the intrinsic parameter matrix of the actual camera 
        (when running <code>ros2 topics echo turtlebot3/camera/camera_info</code>) does not match what is indicated in the guide. 
        Below I show the intrinsic parameter matrix calculated according to the guide:
    </p>

    <img src="images/image7.png" alt="Intrinsic matrix from guide" style="max-width: 400px; width: 100%; display: block; margin: 15px auto;">

    <p>
      And here is the matrix <code>K</code> retrieved directly from the ROS2 topic:
    </p>
    
    <img src="images/image8.png" alt="Intrinsic matrix from ROS topic" style="max-width: 400px; width: 100%; display: block; margin: 15px auto;">
    

    <p>
      The calibration difference is not significant enough to fully explain the observed errors in the pose estimation.
      So, while I have applied the correction, I continued investigating other potential issues in the process.
    </p>

    <h2>üß† Initial Diagnosis</h2>
    <p>
      Since the first attempt, the estimated position of the robot didn‚Äôt align with its real location in the simulated map.
      Even after correcting the camera intrinsic matrix, the estimated pose was still wrong.
      This indicated a deeper conceptual issue: the transformations between coordinate systems were not being composed correctly.
    </p>

    <h2>üìâ Nature of the Error</h2>
    <p>
      Upon close inspection, I realized that <code>solvePnP</code> estimates the tag's pose in the camera‚Äôs optical coordinate system, where:
    </p>
    <ul>
      <li>Z points forward</li>
      <li>X points right</li>
      <li>Y points down</li>
    </ul>
    <p>
      This differs from the physical coordinate system of the AprilTag and the robot, where:
    </p>
    <ul>
      <li>X points forward</li>
      <li>Y points left</li>
      <li>Z points up</li>
    </ul>

    <p>
      Ignoring these differences led to misaligned and misoriented pose estimations. To correct this, I built a transformation chain that includes:
    </p>

    <ol>
      <li>
        <strong>Physical Tag ‚Üí Optical Tag</strong><br>
        First, a fixed transformation is applied to adapt the physical tag's frame to OpenCV‚Äôs expected optical frame:
        <pre><code>T_tag_to_tag_optical = np.array([
    [ 0,  0, 1, 0],
    [-1,  0, 0, 0],
    [ 0, -1, 0, 0],
    [ 0,  0, 0, 1]
])</code></pre>
      </li>
      <li>
        <strong>Optical Tag ‚Üí Optical Camera</strong><br>
        Obtained from <code>solvePnP</code> and inverted to express the camera relative to the tag.
      </li>
      <li>
        <strong>Optical Camera ‚Üí Physical Camera</strong><br>
        Inverted transformation to move from the optical to the physical system of the camera.
      </li>
      <li>
        <strong>Camera ‚Üí Robot Center</strong><br>
        The physical offset from the camera to the center of the TurtleBot, based on the robot‚Äôs model:
        <pre><code>T_cam_to_robot = np.array([
    [1, 0, 0,  0.069],
    [0, 1, 0, -0.047],
    [0, 0, 1,  0.107],
    [0, 0, 0,  1]
])</code></pre>
      </li>
      <li>
        <strong>World ‚Üí Physical Tag</strong><br>
        Built using the tag‚Äôs pose (position + yaw) from the <code>apriltags_poses.yaml</code> file.
      </li>
    </ol>

    <h2>üìè Tag Size Correction</h2>
    <p>
      I had previously noticed this but overlooked its importance: the full tag size is 0.3, but I was calculating the corners 
      based on the black square inside the tag, which is only 0.24. Correcting this parameter was crucial for the final accuracy of the pose estimation.
    </p>

    <h2>‚úÖ Results after changes</h2>
    <p>
      With this corrected approach, the robot now appears exactly at its real position from the very first frame, 
      and the estimated orientation aligns consistently with the camera‚Äôs perspective. The following image demonstrates this:
    </p>

    <img src="images/image9.png" alt="Robot correctly positioned on the map">

    <p>
      The next step will be to start moving the robot and incorporate odometry for localization when no tags are visible.
    </p>

    <p class="back"><a href="index.html">‚¨Ö Back to Exercise 3</a></p>
  </div>
</body>
</html>
