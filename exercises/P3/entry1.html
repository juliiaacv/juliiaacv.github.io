<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Exercise 3 | Entry 1</title>
    <style>
        body {
            font-family: 'Verdana', sans-serif;
            max-width: 800px;
            margin: auto;
            padding: 20px;
            background-color: #d2baee;
            line-height: 1.6;
        }
        .container {
            background-color: white;
            padding: 20px;
            border-radius: 10px;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
        }
        h1, h2 {
            color: #450549;
            text-align: center;
        }
        p {
            font-size: 18px;
            text-align: justify;
        }
        img {
            width: 100%;
            max-width: 600px;
            display: block;
            margin: 15px auto;
            border-radius: 10px;
        }
        .image-row {
            display: flex;
            justify-content: center;
            gap: 20px;
            flex-wrap: wrap;
            margin: 20px 0;
        }
        .image-row img {
            width: 48%;
            border-radius: 10px;
        }
        .video-container {
            display: flex;
            justify-content: center;
            margin: 20px 0;
        }
        iframe {
            width: 100%;
            max-width: 600px;
            height: 340px;
            border-radius: 10px;
        }
        .back {
            display: block;
            margin-top: 20px;
            text-align: center;
            font-size: 16px;
            font-weight: bold;
            color: #b82c89;
            text-decoration: none;
        }
        .back:hover {
            color: #b82c89;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>üß≠ Entry 1: AprilTag Detection and Pose Estimation</h1>

        <h2>üöÄ Starting the Exercise</h2>
        <p>
            To begin this exercise, I simply displayed the camera capture from the robot to get an initial sense of the setup.
            When execution starts, the map shows the robot's current position. The front of the robot (where the camera is pointing) is indicated by a small blue triangle, as seen below.
        </p>

        <p>
            Additionally, the captured image is shown. In this case, two AprilTags are visible ‚Äî one closer than the other:
        </p>

        <img src="images/image1.png" alt="Initial camera capture showing AprilTags">

        <h2>üîç AprilTag Detection</h2>
        <p>
            Next, following the exercise guide, I added a function to detect AprilTags in the image using the <code>pyapriltags</code> library, 
            specifying the ‚Äútag36h11‚Äù family as indicated.
        </p>

        <p>
            The detector returns a list of detected tags in the image, each with the following information:
        </p>

        <ul>
            <li><strong>tag_family:</strong> The tag family. In this case, "tag36h11".</li>
            <li><strong>tag_id:</strong> An integer representing the unique identifier of the tag within its family.</li>
            <li><strong>tag_size:</strong> The real-world size of the AprilTag. This is returned as <code>None</code> because no size was specified in the detector.
                According to the guide: <em>"The AprilTags are set at 0.8 meters in height and have a size of 0.3 x 0.3 meters (the black part is 0.24 x 0.24 meters)."</em>
            </li>
            <li><strong>hamming:</strong> The number of differing bits between the detected tag and the expected code. It is zero.</li>
            <li><strong>decision_margin:</strong> A value representing the confidence of the detection.</li>
            <li><strong>homography:</strong> A 3x3 matrix representing the projective transformation from the tag‚Äôs coordinate system to the image.</li>
            <li><strong>center:</strong> A tuple of two values (x, y) representing the center of the tag in pixel coordinates within the image.</li>
            <li><strong>corners:</strong> A list of four (x, y) points representing the corners of the detected tag, in the following order:
                <ul>
                    <li>Top-left corner</li>
                    <li>Top-right corner</li>
                    <li>Bottom-right corner</li>
                    <li>Bottom-left corner</li>
                </ul>
            </li>
            <li><strong>pose_R:</strong> A 3x3 rotation matrix representing the orientation of the tag in 3D space.</li>
            <li><strong>pose_t:</strong> A 3x1 translation vector representing the position of the tag relative to the camera.</li>
            <li><strong>pose_err:</strong> The error associated with the pose estimation.</li>
        </ul>

        <p>
            By drawing the lines between the corners and highlighting the tag center, the result is visualized as follows:
        </p>

        <img src="images/image2.png" alt="Detected AprilTags with corner and center lines drawn">

        <h2>üîÑ Tag Detection at Various Angles</h2>
        <p>
            The initial image is taken with the camera directly facing the tags, but I wanted to test how well the system performs under different conditions.
        </p>

        <p>
            After taking a short random walk around the environment, I observed that even when the AprilTag doesn‚Äôt appear as a perfect square,
            it is still correctly detected. Below are some images that illustrate this:
        </p>

        <div class="image-row">
            <img src="images/image3.png" alt="AprilTag seen at angle 1">
            <img src="images/image4.png" alt="AprilTag seen at angle 2">
        </div>

        <p>
            In the next image, two tags appear: one far away, which is successfully detected, and another on the left side,
which appears nearly as a thin line due to its extreme angle ‚Äî and, as expected, is not detected correctly:
        </p>

        <img src="images/image5.png" alt="Side tag not detected properly due to angle">

        <p>
            Additionally, when the AprilTag is partially occluded (roughly when half of it is out of view), detection fails.
            This can be clearly seen in the video below: the tag is detected until it exits the right side of the frame.
        </p>

        <div class="video-container">
            <iframe src="https://www.youtube.com/embed/EQusKhfRO-c" frameborder="0" allowfullscreen></iframe>
        </div>

        <h2>üìç Getting the Tag's Coordinates</h2>
        <p>
            Before estimating the robot‚Äôs pose relative to the tag, I visualized the available 3D coordinates of the tag's center along with its 2D image position.
            Using the provided YAML data and the detected tag IDs, I retrieved their 3D coordinates in the simulated world.
        </p>

        <p>
            I created a helper function to visualize both 2D and 3D coordinates for debugging:
        </p>

        <img src="images\image6.png" alt="3D world and 2D image coordinates of the tag center">

        <p>
            The 3D coordinates refer to the center of the tag in the simulated environment, while the 2D coordinates are from the image.
            These values will be useful later when transforming the robot‚Äôs pose to world coordinates using <code>solvePnP</code>.
        </p>

        <h2>üìê SolvePnP</h2>
        <p>
            Leaving the previous section aside, the next logical step after detecting the AprilTag is to estimate the robot‚Äôs pose with respect to the tag‚Äôs reference frame.
        </p>
        
        <p>
            That is precisely what I address here. As mentioned, the pose estimation is performed relative to the AprilTag‚Äôs coordinate system, 
            where the center is considered to be at (0, 0), and the corners are calculated based on this origin and the tag‚Äôs size 
            (as provided in the exercise instructions, 0.3 √ó 0.3 m total, 0.24 √ó 0.24 m black region).
        </p>

        <p>
            Additionally, the corner coordinates detected by the AprilTag detector are needed, along with the camera intrinsic matrix, 
            which is obtained as described in the guide.
        </p>

        <p>
            With all of this information, I can now apply the <code>solvePnP</code> function, which returns:
        </p>

        <ul>
            <li><strong>tvec:</strong> Camera position relative to the tag</li>
            <li><strong>rvec:</strong> Rotation vector (Rodrigues format) relative to the tag</li>
        </ul>

        <p>
            In the video I show below you can see that the operation is performed properly whenever a tag is detected, having some variations with respect to the same tag when the robot changes position, but remaining very similar between iterations.
            On the other hand, you can see the captured image, in this case showing two apriltags, one closer than the other:
        </p>

        <div class="video-container">
            <iframe src="https://www.youtube.com/embed/D05lbqzSD0Y" frameborder="0" allowfullscreen></iframe>
        </div>

        <p class="back"><a href="../index.html">‚¨Ö Back to Exercise 3</a></p>
    </div>
</body>
</html>
